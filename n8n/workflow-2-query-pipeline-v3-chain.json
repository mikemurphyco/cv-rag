{
  "name": "CV-RAG #2: Query-Pipeline (v3.0 - Chain-Based)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "cv-rag-query",
        "responseMode": "responseNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -800,
        0
      ],
      "id": "webhook-node",
      "name": "Webhook",
      "webhookId": "cv-rag-query-webhook-v3"
    },
    {
      "parameters": {
        "mode": "load",
        "prompt": "={{ $json.chatInput }}",
        "tableName": "cv_chunks",
        "topK": 5,
        "options": {
          "columnNames": {
            "values": {
              "contentColumnName": "content"
            }
          }
        }
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePGVector",
      "typeVersion": 1.3,
      "position": [
        -600,
        0
      ],
      "id": "vector-store-node",
      "name": "Postgres Vector Store",
      "credentials": {
        "postgres": {
          "id": "TZKsgiv75reWBifW",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "model": "nomic-embed-text:latest"
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "typeVersion": 1,
      "position": [
        -600,
        200
      ],
      "id": "embeddings-node",
      "name": "Embeddings Ollama",
      "credentials": {
        "ollamaApi": {
          "id": "UTJsPhetajaS17pP",
          "name": "Ollama account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Format retrieved chunks into context for the LLM\nconst items = $input.all();\n\n// Extract the user's question from the first item\nconst userQuestion = items[0].json.chatInput;\n\n// Combine all retrieved document chunks into context\nconst contextChunks = items.map((item, index) => {\n  const content = item.json.content || item.json.pageContent || '';\n  return `[Chunk ${index + 1}]\\n${content}`;\n}).join('\\n\\n');\n\n// Build the system prompt and user message\nconst systemPrompt = `You are an AI assistant answering questions about Mike Murphy based on his resume and professional materials. Answer using ONLY the information provided in the context. If the context doesn't contain relevant information, say \"I don't have that information in Mike's resume\". Be specific and concise.`;\n\nconst userMessage = `CONTEXT FROM RESUME:\\n${contextChunks}\\n\\nQUESTION: ${userQuestion}`;\n\nreturn [{\n  json: {\n    model: \"llama3.2:latest\",\n    messages: [\n      {\n        role: \"system\",\n        content: systemPrompt\n      },\n      {\n        role: \"user\",\n        content: userMessage\n      }\n    ],\n    stream: false,\n    options: {\n      temperature: 0.3,\n      top_p: 0.9\n    }\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -400,
        0
      ],
      "id": "format-context-node",
      "name": "Format Context"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://158.220.127.4:11434/api/chat",
        "authentication": "none",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json) }}",
        "options": {
          "timeout": 60000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -200,
        0
      ],
      "id": "ollama-request-node",
      "name": "Call Ollama API"
    },
    {
      "parameters": {
        "jsCode": "// Extract the LLM response from Ollama API\nconst items = $input.all();\nconst response = items[0].json;\n\n// Ollama chat API returns: {\"message\": {\"role\": \"assistant\", \"content\": \"...\"}}\nconst answer = response.message?.content || 'No response generated';\n\nreturn [{\n  json: {\n    answer: answer.trim()\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        0,
        0
      ],
      "id": "format-response-node",
      "name": "Format Response"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        200,
        0
      ],
      "id": "respond-node",
      "name": "Respond to Webhook"
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Postgres Vector Store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Postgres Vector Store": {
      "main": [
        [
          {
            "node": "Format Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings Ollama": {
      "ai_embedding": [
        [
          {
            "node": "Postgres Vector Store",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Format Context": {
      "main": [
        [
          {
            "node": "Call Ollama API",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama API": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "7a3f4b30d139bb4d4da62ccf6bec8aefe503a6057c743693063b0411d7afe9d0"
  },
  "tags": [
    {
      "updatedAt": "2025-11-05T20:36:10.321Z",
      "createdAt": "2025-11-05T20:36:10.321Z",
      "id": "UXBoTIXaXgl3iGpi",
      "name": "RAG"
    }
  ]
}
